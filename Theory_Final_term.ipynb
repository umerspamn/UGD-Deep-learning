{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkJ2+jh44TFMD+yhM6XPp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umerspamn/UGD-Deep-learning/blob/main/Theory_Final_term.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name:Umer Farooq\n",
        "course:Data Science"
      ],
      "metadata": {
        "id": "wo6vvwgunQU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) what do understand by the term Activation function? Name any 4 Activation\n",
        "functions."
      ],
      "metadata": {
        "id": "iDDQ0bK2ndWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An activation function is a mathematical function in the artificial neural network that calculates the output of a neuron or node. In essence, it defines if the neuron should be activated based on the calculation of the weighted sum and the addition of bias. An activation function introduces non-linearity into the network to learn complex patterns and make decisions.\n",
        "\n",
        "Activation functions perform several different roles in simple neural networks. First, they introduce non-linearity in the model, allowing it to describe a very complex input-output relationship, which would be severely restricted without it. This may help to stabilize the learning process, as non-linear functions can map outputs into a specific range. Third, they change the gradient flow during backpropagation, causing a change in the flow of weight updating."
      ],
      "metadata": {
        "id": "jEnCEvDWnyGG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbR0j2lsn24J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Differentiate between Data Augmentation and Synthetic Data Approach."
      ],
      "metadata": {
        "id": "EZBhyekRraiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both data augmentation and synthetic data are commonly used methods to improve machine-learning models, but their methods and applications differ significantly. Data augmentation refers to the modification of existing records in a training data set, thereby making it more diverse than the original data. Such changes include various transformations, such as rotation, scaling, flipping, and color adjustments.\n",
        "\n",
        "Conversely, synthetic data approaches involve creating entirely new data that resembles real-world data. This process can be done through computer simulations, generative adversarial networks (GANs), and other model-generating techniques. Such data is useful when actual data is scarce, sensitive, or difficult to obtain."
      ],
      "metadata": {
        "id": "vEhTUtY5rcW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the terms Forward propagation and Backward Propagation in Deep\n",
        "Learning"
      ],
      "metadata": {
        "id": "Ny49AkuQrk87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward and backward propagation are fundamental concepts in deep learning used during the training of neural networks.\n",
        "\n",
        "Forward propagation feeds input data into the neural network to produce an output. It passes input through the entire network, involving operations such as matrix multiplication and activation functions across multiple layers of neurons. The input data is transformed layer by layer until the final output is obtained. Forward propagation is the process through which a decision can be made or a prediction developed based on provided input data.\n",
        "\n",
        "Backward propagation, or backpropagation, is a mechanism where the errors in the output are used to adjust the weights of the neurons. After forward propagation, the network's output is compared with the actual target values to assess the error. By applying the chain rule, we compute the gradient of the loss function with respect to each weight. These gradients are used to update the weights, reducing the error and improving the model's accuracy."
      ],
      "metadata": {
        "id": "qcutNmHKskTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Differentiate between Machine Learning & Deep Learning.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n0gdY2a4tSdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many differences in machine learning and deep learning. some of them are as follows:\n",
        "Machine Learning is like teaching a computer to recognize patterns in data by giving it examples and letting it figure things out on the other hand In deep learning Instead of just recognizing patterns, the computer tries to mimic how our brain works, with layers of interconnected \"neurons\" to learn even more complex stuff from raw data like images or text."
      ],
      "metadata": {
        "id": "l99CMzdVtSgK"
      }
    }
  ]
}